{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"gan.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"pJuuEVy2TTfd","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1624973104767,"user_tz":-480,"elapsed":21455,"user":{"displayName":"邱柏皓","photoUrl":"","userId":"05225412124819926131"}},"outputId":"f2861334-2eae-4f85-e525-9a8e4ea8dbab"},"source":["from google.colab import drive\n","import os\n","drive.mount('/content/gdrive')\n","os.chdir(\"/content/gdrive/Shared drives/gan/\")\n","os.getcwd()"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/gdrive/Shared drives/gan'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3wkVpgFVtVwx","executionInfo":{"status":"ok","timestamp":1624700502037,"user_tz":-480,"elapsed":1093,"user":{"displayName":"邱柏皓","photoUrl":"","userId":"05225412124819926131"}},"outputId":"07086a75-1210-44fa-d695-6d61ece7ccfe"},"source":["!python --version\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Python 3.7.10\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eRCR8CWPtrhd","executionInfo":{"status":"ok","timestamp":1624700978509,"user_tz":-480,"elapsed":272,"user":{"displayName":"邱柏皓","photoUrl":"","userId":"05225412124819926131"}},"outputId":"05f501d3-2542-492e-a308-8dfd38cedd14"},"source":["!apt-get install python3.6"],"execution_count":null,"outputs":[{"output_type":"stream","text":["nohup: appending output to 'nohup.out'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"42MDla_dqWzh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624973190753,"user_tz":-480,"elapsed":81929,"user":{"displayName":"邱柏皓","photoUrl":"","userId":"05225412124819926131"}},"outputId":"9bfe7cea-10ac-40b0-fefa-c7f3478b9b4a"},"source":["!python3 train_all.py \\\n","--dataroot \"data_image/A\" \\\n","--checkpoint \"model_checkpoint/A\" \\\n","--real_image \"real_image/A\" \\\n","--fake_image \"fake_image/A\" \\\n","--niter 1 \\\n","--batch_size 16 \\\n","--workers 4 \\\n","--image_size 512 \\\n","--glr 0.0001 \\\n","--dlr 0.000005 \\\n","--bl_D_x 0.5 \\\n","--bl_D_G_z1 0.5 \\\n","--bl_D_G_z2 0.5 \\\n","--ngf 64 \\\n","--ndf 64 \\\n","--G_pth 1 \\\n","--D_pth 1 \\"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Random Seed: 3770\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","Generator(\n","  (main): Sequential(\n","    (Conv(1)): ConvTranspose2d(100, 4096, kernel_size=(4, 4), stride=(1, 1), bias=False)\n","    (BatchNorm(1)): BatchNorm2d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (ReLU(1)): ReLU(inplace=True)\n","    (Conv(2)): ConvTranspose2d(4096, 2048, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (BatchNorm(2)): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (ReLU(2)): ReLU(inplace=True)\n","    (Conv(3)): ConvTranspose2d(2048, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (BatchNorm(3)): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (ReLU(3)): ReLU(inplace=True)\n","    (Conv(4)): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (BatchNorm(4)): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (ReLU(4)): ReLU(inplace=True)\n","    (Conv(5)): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (BatchNorm(5)): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (ReLU(5)): ReLU(inplace=True)\n","    (Conv(6)): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (BatchNorm(6)): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (ReLU(6)): ReLU(inplace=True)\n","    (Conv(7)): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (BatchNorm(7)): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (ReLU(7)): ReLU(inplace=True)\n","    (Conv(8)): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (Tanh(8)): Tanh()\n","  )\n",")\n","Discriminator(\n","  (main): Sequential(\n","    (Conv(1)): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (LeakyReLU(1)): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (Conv(2)): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (BatchNorm(2)): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (LeakyReLU(2)): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (Conv(3)): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (BatchNorm(3)): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (LeakyReLU(3)): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (Conv(4)): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (BatchNorm(4)): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (LeakyReLU(4)): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (Conv(5)): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (BatchNorm(5)): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (LeakyReLU(5)): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (Conv(6)): Conv2d(1024, 2048, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (BatchNorm(6)): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (LeakyReLU(6)): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (Conv(7)): Conv2d(2048, 4096, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (BatchNorm(7)): BatchNorm2d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (LeakyReLU(7)): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (Conv(8)): Conv2d(4096, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n","    (Sigmoid(8)): Sigmoid()\n","  )\n",")\n","Traceback (most recent call last):\n","  File \"train_all.py\", line 357, in <module>\n","    main()\n","  File \"train_all.py\", line 291, in main\n","    errD_real = criterion(output, label)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\", line 612, in forward\n","    return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\", line 2893, in binary_cross_entropy\n","    return torch._C._nn.binary_cross_entropy(input, target, weight, reduction_enum)\n","RuntimeError: Found dtype Long but expected Float\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Bo9lOiuDTIfJ"},"source":["!python3 train_all.py \\\n","--dataroot \"data_image\" \\\n","--checkpoint \"model_checkpoint/all\" \\\n","--real_image \"real_image/all\" \\\n","--fake_image \"fake_image/all\" \\\n","--niter 1 \\\n","--batch_size 16 \\\n","--workers 4 \\\n","--image_size 512 \\\n","--glr 0.0004 \\\n","--dlr 0.00001 \\\n","--bl_D_x 0.6 \\\n","--bl_D_G_z1 0.4 \\\n","--bl_D_G_z2 0.6 \\\n","--ngf 64 \\\n","--ndf 64 \\\n","--G_pth 1 \\\n","--D_pth 1 \\"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NTzKqrQLUNlY","colab":{"base_uri":"https://localhost:8080/","height":779},"executionInfo":{"status":"ok","timestamp":1594023926030,"user_tz":-480,"elapsed":9922,"user":{"displayName":"邱柏皓","photoUrl":"","userId":"17348084347466297617"}},"outputId":"c5c2f5b4-58e2-4732-cf61-f1a19204e884"},"source":["!python3 train_all.py -h"],"execution_count":null,"outputs":[{"output_type":"stream","text":["usage: train_all.py [-h] [--test] [--manual_seed MANUAL_SEED]\n","                    [--dataroot DATAROOT] [--image_size IMAGE_SIZE]\n","                    [--batch_size BATCH_SIZE] [--workers WORKERS] [--ngf NGF]\n","                    [--extra_layers EXTRA_LAYERS] [--nz NZ] [--G_pth G_PTH]\n","                    [--checkpoint CHECKPOINT] [--ndf NDF] [--D_pth D_PTH]\n","                    [--dlr DLR] [--beta1 BETA1] [--glr GLR] [--niter NITER]\n","                    [--bl_D_x BL_D_X] [--bl_D_G_z1 BL_D_G_Z1]\n","                    [--bl_D_G_z2 BL_D_G_Z2] [--real_image REAL_IMAGE]\n","                    [--fake_image FAKE_IMAGE]\n","\n","optional arguments:\n","  -h, --help            show this help message and exit\n","  --test                want to try something but don't want to cover old\n","                        files\n","  --manual_seed MANUAL_SEED\n","                        manual seed\n","  --dataroot DATAROOT   path to dataset\n","  --image_size IMAGE_SIZE\n","                        the height / width of the input image to network,\n","                        default=64\n","  --batch_size BATCH_SIZE\n","                        input batch size, default=64\n","  --workers WORKERS     number of data loading workers, default=2\n","  --ngf NGF             number of filters in the generator, default=64\n","  --extra_layers EXTRA_LAYERS\n","                        extra layers, default=0\n","  --nz NZ               size of the latent z vector, default=100\n","  --G_pth G_PTH         index of path to Generator net (to continue training)\n","  --checkpoint CHECKPOINT\n","                        folder to output model checkpoints\n","  --ndf NDF             number of filters in the discriminator, default=64\n","  --D_pth D_PTH         index of path to Discriminator net (to continue\n","                        training)\n","  --dlr DLR             discriminator learning rate, default=0.0002\n","  --beta1 BETA1         beta1 for adam. default=0.5\n","  --glr GLR             generator learning rate, default=0.0002\n","  --niter NITER         number of epochs to train for, default=25\n","  --bl_D_x BL_D_X       balance value for D_x, default=0.5\n","  --bl_D_G_z1 BL_D_G_Z1\n","                        balance value for D_G_z1, default=0.5\n","  --bl_D_G_z2 BL_D_G_Z2\n","                        balance value for D_G_z2, default=0.5\n","  --real_image REAL_IMAGE\n","                        folder to output Real images\n","  --fake_image FAKE_IMAGE\n","                        folder to output Fake images\n"],"name":"stdout"}]}]}